{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://bernardmarr.com/img/What%20is%20an%20Artificial%20Neural%20Networks.jpg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2025\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://bernardmarr.com/img/What%20is%20an%20Artificial%20Neural%20Networks.jpg</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Redes Neuronales</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales profundas (deep learning) han ganado mucha popularidad en los últimos años, especialmente para problemas complejos, como el reconocimiento de imágenes, el procesamiento del lenguaje natural y otras aplicaciones que requieren modelos potentes. \n",
    "\n",
    "Sin embargo, las redes neuronales básicas también pueden usarse en problemas de aprendizaje supervisado más simples, como regresión o clasificación, AUNQUE hay varias razones por las cuales otros algoritmos son más comunes en estos casos...\n",
    "\n",
    "1. Complejidad computacional\n",
    "2. Sobreajuste (Overfitting)\n",
    "3. Necesidad de más datos\n",
    "4. Tuning de Hiperparámetros (las redes neuronales tienen muchos hiperparámetros)\n",
    "5. Interpretabilidad, son modelos de caja negra\n",
    "\n",
    "**¿Cuándo se utilizan redes neuronales en aprendizaje supervisado?**\n",
    "\n",
    "Aunque no siempre son la primera opción para problemas simples, las redes neuronales pueden ser muy útiles en problemas supervisados más complejos donde:\n",
    "\n",
    "- Hay gran cantidad de datos disponibles\n",
    "- Los problemas son no lineales\n",
    "- Hay tareas de percepción involucradas (como visión por computadora, reconocimiento de voz o procesamiento de lenguaje natural), donde las redes neuronales profundas sobresalen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la caja negra...\n",
    "\n",
    "**¿Qué es una neurona?**\n",
    "\n",
    "Es un nombre elegante que le pusieron para referirse a una función en los años de 1940-1950. Se creeía que los nodos eran como neuronas del cerebro.  \n",
    "\n",
    "**¿Qué es una función?**\n",
    "\n",
    "Es un nombre elegante que pusieron para referirse a algo que toma cosas de entrada, aplica alguna lógica y saca un resultado. \n",
    "\n",
    "**¿Qué es una red neuronal?**\n",
    "\n",
    "Es la interconexión de múltiples neuronas (funciones) para resolver un problema complejo que una sola neurona no podría. Dentro de los componentes existen *nodos* y *conexiones*\n",
    "\n",
    "**Pasos que hace la red neuronal:**\n",
    "\n",
    "1. Combinación de las variables de entrada (X). Cada señal recibida tiene una constante w (peso/weight) que permite manipular la importancia de cada variable de entrada. \n",
    "2. Neurona de respuesta. Considerando todas las variables de entrada, la neurona genera una señal de respuesta a través de una función. \n",
    "\n",
    "**Tipos de capas en la red neuronal**\n",
    "\n",
    "- Capa de entrada: capa que toma los datos de entrada\n",
    "- Capas ocultas: toman las entradas de otra capa y pasan a salida hacia otra capa (son intermediarias entre la entrada y la salida)\n",
    "- Capas de salida: capa que hace la predicción\n",
    "\n",
    "<img style=\"float: center; margin: 0px 0px 0px 0px;\" src=\"https://www.smartsheet.com/sites/default/files/IC-simplified-artificial-neural-networks-corrected.svg\" width=\"350px\" height=\"180px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Funciones de activación**\n",
    "\n",
    "El comportamiento de las neuronas de las redes neuronales es basado en la función en la que la neurona se especializó. Una neurona puede cambiar su respuesta dependiendo de la función de activación $\\varphi(*)$ que sea seleccionada\n",
    "\n",
    "Funciones comunes:\n",
    "1. Lineal: $\\varphi(\\nu)=\\nu$\n",
    "2. Sigmoidal: $\\varphi(\\nu)=\\frac{e^{\\nu}}{1+e^{\\nu}}$ \n",
    "3. Tanh: $\\varphi(\\nu)=\\frac{e^{\\nu}-e^{-\\nu}}{e^{\\nu}+e^{-\\nu}}=tanh(\\nu)$\n",
    "4. Softplus: $\\varphi(\\nu)=log(1+e^{\\nu})$\n",
    "5. RELU: $\\varphi(\\nu)=max(0,\\nu)$\n",
    "\n",
    "Las funciones de activación suelen ser diferenciables, lo que significa que la derivada de primer orden se puede calcular para un valor de entrada dado. Esto es necesario dado que las redes neuronales generalmente se entrenan utilizando el algoritmo de backpropagation que requiere la derivada del error de predicción para actualizar los pesos del modelo.\n",
    "\n",
    "**¿Cómo elegir la función de activación para las capas ocultas?**\n",
    " \n",
    "- RELU: es la más utilizada para capas ocultas. Es fácil de calcular, lo que la hace más rápida para entrenar. Es simple de implementar y no es tan susceptible a gradientes que se desvanecen\n",
    "- Leaky RELU: Similar a ReLU, pero permite valores negativos pequeños cuando x≤0, lo que evita que las neuronas queden completamente inactivas.\n",
    "- Evita Sigmoid y Tanh en las capas ocultas\n",
    "\n",
    "**¿Cómo elegir la función de activación para las capas de salida?**\n",
    "\n",
    "- Lineal: Para problemas de regresión donde la salida es un valor continuo.\n",
    "- Sigmoidal: típicamente para problemas de clasificación binaria.\n",
    "- Softmax: Para problemas de clasificación multiclase\n",
    "\n",
    "\n",
    "**¿Cómo son las redes neuronales para regresión?**\n",
    "\n",
    "$$\\nu^{1} = w_{0}^{1}+w^{1}$$\n",
    "$$y^{1} = \\varphi(\\nu^{1})$$\n",
    "$$\\nu^{2} = w_{0}^{2}+w^{2}y_{1}$$\n",
    "<font color= #2E9AFE>$$y^{2} = \\varphi(\\nu^{2})$$</font>\n",
    "\n",
    "La única diferencia es la última salida. La función de la salida tiene que ser **lineal**\n",
    "\n",
    "\n",
    "**¿Cómo elegir cuántas capas ocultas?**\n",
    "\n",
    "- Problemas sencillos: Una sola capa oculta suele ser suficiente para capturar patrones simples.\n",
    "- Problemas más complejos (con relaciones no lineales más complicadas): Dos capas ocultas pueden ser útiles \n",
    "\n",
    "En la práctica, la mayoría de los problemas sencillos no necesitan más de 1 o 2 capas ocultas.\n",
    "\n",
    "**¿Cómo elegir cuántas neuronas en las capas ocultas?** \n",
    "\n",
    "- Un buen punto de partida es comenzar con un número de neuronas que sea similar o cercano al número de características de entrada.\n",
    "- Para problemas simples, puedes comenzar con entre 4 y 16 neuronas por capa.\n",
    "- Si los datos tienen una cantidad moderada de complejidad o son no lineales, podrías aumentar a 32 o 64 neuronas en la capa oculta.\n",
    "- Evita tener muchas más neuronas que características de entrada, ya que eso puede llevar a un sobreajuste.\n",
    "- En redes con múltiples capas ocultas, una estrategia común es que las capas posteriores tengan menos neuronas que las capas iniciales. Esto es porque las primeras capas extraen más características, mientras que las capas más profundas se enfocan en combinarlas.\n",
    "\n",
    "**Recomendaciones**\n",
    "\n",
    "- Se recomienda hacer un pre-procesamiento de los datos - Escalamiento\n",
    "- Comúnmente todas las capas ocultas usan la misma función de activación\n",
    "- La función de salida generalmente usa diferente función de activación que el de las capas ocultas. \n",
    "- En la experiencia, conviene más aumentar el número de neuronas ocultas en lugar de aumentar el número de capas ocultas.\n",
    "- Se recomienda no usar muchas capas ocultas\n",
    "\n",
    "\n",
    "**¿Por qué el deep learning no funciona tan bien en problemas clásicos de regresión/clasificación?**\n",
    "\n",
    "Gran parte del éxito de las redes neuronales modernas proviene de su capacidad para explotar la naturaleza compositiva del mundo. Es decir, en problemas de percepción como el análisis de imágenes o audio, las características tienen un orden (espacial o temporal) y los patrones locales se agregan para formar conceptos y objetos de nivel superior (por ejemplo, la imagen de un automóvil está hecha de ruedas y otras partes, que están hechas de características visuales de nivel inferior, que están hechas de formas básicas como bordes, círculos y líneas, etc.). \n",
    "\n",
    "Las redes neuronales modernas, como las redes neuronales convolucionales (CNN), aprovechan esto aprendiendo características cada vez más abstractas en las capas más profundas.\n",
    "\n",
    "Por el contrario, los problemas de regresión clásicos constan de una serie de características no ordenadas, y el valor objetivo se puede predecir bastante bien con un modelo lineal/no lineal superficial de las características de entrada. En cierto sentido, esta propiedad composicional presente en problemas como la clasificación de imágenes o el reconocimiento de voz no está presente en problemas como \"Predecir los ingresos de un individuo en función de su sexo, edad, nacionalidad, grado académico, tamaño de la familia...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Eficiencia de Energia\n",
    "\n",
    "Se busca analizar la eficiencia energética de los edificios.\n",
    "\n",
    "Se realizaron análisis energéticos utilizando 12 tipos de edificios diferentes simulados en Ecotect.\n",
    "\n",
    "Los edificios se diferencian en cuanto a la superficie con cristales, la distribución del área acristalada y la orientación, entre otros parámetros. \n",
    "\n",
    "Se simularon varios escenarios en función de las características antes mencionadas para obtener 768 formas de edificios. \n",
    "\n",
    "El conjunto de datos comprende 768 muestras y 8 características, con el objetivo de predecir dos respuestas con valor real.\n",
    "\n",
    "Información de atributos:\n",
    "- X1 Relative Compactness\n",
    "- X2 Surface Area\n",
    "- X3 Wall Area\n",
    "- X4 Roof Area\n",
    "- X5 Overall Height\n",
    "- X6 Orientation\n",
    "- X7 Glazing Area\n",
    "- X8 Glazing Area Distribution\n",
    "\n",
    "Targets:\n",
    "- Y1: Heating Load (in Wh)\n",
    "- Y2: Cooling Load (in Wh)\n",
    "- \n",
    "Las características son numéricas y representan diversas características físicas del edificio, mientras que las variables objetivo representan la energía necesaria para calefacción y enfriamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ENB2012_data.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de correlaciones\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlaciones con el target\n",
    "correlation_target = df.corr()[['Y1','Y2']]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_target, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribucion de las varialbes\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar una red compartida con dos salidas (cooling and heating load), ya que los dos targets están físicamente relacionados y la red puede aprender representaciones compartidas.\n",
    "\n",
    "Algunos tips para empezar la arquitectura inicial de la red:\n",
    "\n",
    "**a. Entrada y salida**\n",
    "\n",
    "- Entradas: 8 características → tamaño de capa de entrada = 8\n",
    "- Salidas: 2 objetivos → tamaño de capa de salida = 2\n",
    "\n",
    "**b. Capas ocultas**\n",
    "\n",
    "Por lo general, comienzas poco a poco y vas ampliando:\n",
    "\n",
    "- Comienza con 1 a 2 capas ocultas.\n",
    "- Numero de neuronas: entre el tamaño de entrada y salida (8) y tal vez entre 2 y 4 veces más (16–32–64).\n",
    "\n",
    "Por ejemplo:\n",
    "Input (8)\n",
    "↓\n",
    "Dense(64, ReLU)\n",
    "↓\n",
    "Dense(32, ReLU)\n",
    "↓\n",
    "Dense(2, Linear)\n",
    "\n",
    "**c. Funcion de activacion**\n",
    "\n",
    "Comenzar con RELU para capas ocultas\n",
    "\n",
    "**d. Agregar Regularizacion**\n",
    "\n",
    "Agregar regularizacion si vemos overfitting (dropout)\n",
    "\n",
    "**e. Funcion de costo**\n",
    "\n",
    "MSE\n",
    "\n",
    "**f. Optimizer**\n",
    "\n",
    "Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']].values\n",
    "y1 = df['Y1'].values  # Heating Load\n",
    "y2 = df['Y2'].values  # Cooling Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizamos los datos\n",
    "scaler=StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa de entrada (8 variables)\n",
    "input_layer = Input(shape=(X_train.shape[1],), name='input_layer')\n",
    "\n",
    "# Capas ocultas compartidas\n",
    "layer_dense1_shared=Dense(64 , activation='relu', name='shared_dense_1')(input_layer)\n",
    "layer_dense2_shared=Dense(32,activation='relu',name='shared_dense_2')(layer_dense1_shared)\n",
    "layer_dropout1_shared=Dropout(0.2,name='shared_dropout_1')(layer_dense2_shared)\n",
    "\n",
    "#Estructura para Y1 (Heating Load)\n",
    "y1_dense_1=Dense(32, activation='relu',name='y1_dense_1')(layer_dropout1_shared)\n",
    "y1_dropout = Dropout(0.2, name='y1_dropout')(y1_dense_1)\n",
    "y1_dense2 = Dense(16, activation='relu', name='y1_dense_2')(y1_dropout)\n",
    "y1_output = Dense(1, name='y1_output')(y1_dense2)\n",
    "\n",
    "#Estructura para Y2 (Cooling Load)\n",
    "y2_dense1 = Dense(32, activation='relu', name='y2_dense_1')(layer_dropout1_shared)\n",
    "y2_dropout = Dropout(0.2, name='y2_dropout')(y2_dense1)\n",
    "y2_dense3 = Dense(16, activation='relu', name='y2_dense_3')(y2_dropout)\n",
    "y2_output = Dense(1, name='y2_output')(y2_dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Modelo que contenga la capa de entrada y las capas de salida \n",
    "model=Model(inputs=input_layer  ,outputs=[y1_output , y2_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compilamos el modelo\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'y1_output': 'mse', 'y2_output': 'mse'}, \n",
    "              metrics={'y1_output': 'mae', 'y2_output': 'mae'})\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(X_train, [y1_train, y2_train], epochs=200, batch_size=32, \n",
    "                    validation_data=(X_test, [y1_test, y2_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo\n",
    "results = model.evaluate(X_test, [y1_test, y2_test], verbose=0)\n",
    "\n",
    "test_loss, y1_loss, y2_loss, test_mae_heating, test_mae_cooling = results\n",
    "\n",
    "print(f\"Total Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Heating Loss: {y1_loss:.4f}, Cooling Loss: {y2_loss:.4f}\")\n",
    "print(f\"Test MAE for Heating Load: {test_mae_heating:.4f} Wh\")\n",
    "print(f\"Test MAE for Cooling Load: {test_mae_cooling:.4f} Wh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "# ---- Graficamos el loss, que en este caso decidimos que fuera el MSE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Total Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Total Validation Loss')\n",
    "plt.plot(history.history['y1_output_loss'], label='Heating Training Loss')\n",
    "plt.plot(history.history['val_y1_output_loss'], label='Heating Validation Loss')\n",
    "plt.plot(history.history['y2_output_loss'], label='Cooling Training Loss')\n",
    "plt.plot(history.history['val_y2_output_loss'], label='Cooling Validation Loss')\n",
    "plt.title('Model Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graficamos el MAE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['y1_output_mae'], label='Heating Training MAE')\n",
    "plt.plot(history.history['val_y1_output_mae'], label='Heating Validation MAE')\n",
    "plt.plot(history.history['y2_output_mae'], label='Cooling Training MAE')\n",
    "plt.plot(history.history['val_y2_output_mae'], label='Cooling Validation MAE')\n",
    "plt.title('Model MAE over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE (Wh)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "#Como tenemos dos targets, las predicciones hay que separarlas\n",
    "y_train_pred_heating = y_pred_train[0].flatten()\n",
    "y_train_pred_cooling = y_pred_train[1].flatten()\n",
    "\n",
    "y_test_pred_heating = y_pred_test[0].flatten()\n",
    "y_test_pred_cooling = y_pred_test[1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metricas de performance para heating\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(root_mean_squared_error(y_true, y_pred))\n",
    "    return r2, mae, rmse\n",
    "\n",
    "# Metricas train\n",
    "r2_train_heat, mae_train_heat, rmse_train_heat = regression_metrics(y1_train, y_train_pred_heating)\n",
    "r2_train_cool, mae_train_cool, rmse_train_cool = regression_metrics(y2_train, y_train_pred_cooling)\n",
    "print(\"=== Train Performance ===\")\n",
    "print(f\"Heating Load: R²={r2_train_heat:.3f}, MAE={mae_train_heat:.3f}, RMSE={rmse_train_heat:.3f}\")\n",
    "print(f\"Cooling Load: R²={r2_train_cool:.3f}, MAE={mae_train_cool:.3f}, RMSE={rmse_train_cool:.3f}\")\n",
    "\n",
    "# metricas test\n",
    "r2_test_heat, mae_test_heat, rmse_test_heat = regression_metrics(y1_test, y_test_pred_heating)\n",
    "r2_test_cool, mae_test_cool, rmse_test_cool = regression_metrics(y2_test, y_test_pred_cooling)\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"Heating Load: R²={r2_test_heat:.3f}, MAE={mae_test_heat:.3f}, RMSE={rmse_test_heat:.3f}\")\n",
    "print(f\"Cooling Load: R²={r2_test_cool:.3f}, MAE={mae_test_cool:.3f}, RMSE={rmse_test_cool:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_model(input_dim):\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Capas compartidas\n",
    "    x = layers.Dense(128, activation='relu')(input_layer)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Heating\n",
    "    h = layers.Dense(64, activation='relu')(x)\n",
    "    h = layers.Dropout(0.2)(h)\n",
    "    h = layers.Dense(32, activation='relu')(h)\n",
    "    y1 = layers.Dense(1, name='heating_output')(h)\n",
    "    \n",
    "    # Cooling\n",
    "    c = layers.Dense(64, activation='relu')(x)\n",
    "    c = layers.Dropout(0.2)(c)\n",
    "    c = layers.Dense(32, activation='relu')(c)\n",
    "    y2 = layers.Dense(1, name='cooling_output')(c)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=[y1, y2])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import sqrt\n",
    "\n",
    "#KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "r2_heat, r2_cool = [], []\n",
    "mae_heat, mae_cool = [], []\n",
    "rmse_heat, rmse_cool = [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n Fold {fold+1}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train_heat, y_val_heat = df['Y1'][train_idx], df['Y1'][val_idx]\n",
    "    y_train_cool, y_val_cool = df['Y2'][train_idx], df['Y2'][val_idx]\n",
    "    \n",
    "    # Scale inputs\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    # Build & train model\n",
    "    model = build_model(X.shape[1])\n",
    "    model.fit(\n",
    "        X_train, [y_train_heat, y_train_cool],\n",
    "        epochs=80, batch_size=32, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_heat, y_pred_cool = model.predict(X_val, verbose=0)\n",
    "    \n",
    "    # Flatten\n",
    "    y_pred_heat, y_pred_cool = y_pred_heat.flatten(), y_pred_cool.flatten()\n",
    "    \n",
    "    # Compute metrics\n",
    "    r2_heat.append(r2_score(y_val_heat, y_pred_heat))\n",
    "    r2_cool.append(r2_score(y_val_cool, y_pred_cool))\n",
    "    mae_heat.append(mean_absolute_error(y_val_heat, y_pred_heat))\n",
    "    mae_cool.append(mean_absolute_error(y_val_cool, y_pred_cool))\n",
    "    rmse_heat.append(sqrt(mean_squared_error(y_val_heat, y_pred_heat)))\n",
    "    rmse_cool.append(sqrt(mean_squared_error(y_val_cool, y_pred_cool)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Cross-Validation Results ===\")\n",
    "print(f\"Heating Load: R2={np.mean(r2_heat):.3f}±{np.std(r2_heat):.3f}, \"\n",
    "      f\"MAE={np.mean(mae_heat):.3f}, RMSE={np.mean(rmse_heat):.3f}\")\n",
    "print(f\"Cooling Load: R2={np.mean(r2_cool):.3f}±{np.std(r2_cool):.3f}, \"\n",
    "      f\"MAE={np.mean(mae_cool):.3f}, RMSE={np.mean(rmse_cool):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuneo de hiperparametros\n",
    "\n",
    "Especificamente diseniado para redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Serach\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "df = pd.read_csv(\"ENB2012_data.csv\")\n",
    "X_full = df[['X1','X2','X3','X4','X5','X6','X7','X8']].values\n",
    "y1_full = df['Y1'].values\n",
    "y2_full = df['Y2'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_full = scaler.fit_transform(X_full)\n",
    "\n",
    "\n",
    "X_train_full, X_test_full, y1_train_full, y1_test_full, y2_train_full, y2_test_full = train_test_split(\n",
    "    X_full, y1_full, y2_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Definimos el contructor del modelo\n",
    "\n",
    "def build_random_model(params, input_dim):\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # compartidas\n",
    "    x = input_layer\n",
    "    for i in range(params['n_shared_layers']):\n",
    "        x = layers.Dense(params['n_shared_units'], activation='relu')(x)\n",
    "        x = layers.Dropout(params['dropout_rate'])(x)\n",
    "    \n",
    "    # Heating \n",
    "    h = layers.Dense(params['n_shared_units']//2, activation='relu')(x)\n",
    "    y1 = layers.Dense(1, name='heating_output')(h)\n",
    "    \n",
    "    # Cooling \n",
    "    c = layers.Dense(params['n_shared_units']//2, activation='relu')(x)\n",
    "    y2 = layers.Dense(1, name='cooling_output')(c)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=[y1, y2])\n",
    "    \n",
    "    opt = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss={'heating_output': 'mse', 'cooling_output': 'mse'},\n",
    "        metrics={'heating_output': 'mae', 'cooling_output': 'mae'}\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Espacio para el random search\n",
    "\n",
    "param_distributions = {\n",
    "    'n_shared_layers': [1, 2, 3],\n",
    "    'n_shared_units': [32, 64, 128, 256],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'learning_rate': [1e-4, 5e-4, 1e-3, 5e-3],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "\n",
    "n_trials = 5\n",
    "results = []\n",
    "\n",
    "print(\"\\n Empezamos Random Search\")\n",
    "for i in range(n_trials):\n",
    "    params = {k: random.choice(v) for k, v in param_distributions.items()}\n",
    "    print(f\"\\nTrial {i+1}/{n_trials} with params: {params}\")\n",
    "    \n",
    "    X_train_rs, X_val_rs, y1_train_rs, y1_val_rs, y2_train_rs, y2_val_rs = train_test_split(\n",
    "        X_train_full, y1_train_full, y2_train_full, test_size=0.2, random_state=i\n",
    "    )\n",
    "    \n",
    "    model = build_random_model(params, X_train_rs.shape[1])\n",
    "    history = model.fit(\n",
    "        X_train_rs, [y1_train_rs, y2_train_rs],\n",
    "        validation_data=(X_val_rs, [y1_val_rs, y2_val_rs]),\n",
    "        epochs=50, batch_size=params['batch_size'], verbose=0\n",
    "    )\n",
    "    \n",
    "    val_loss = np.mean(history.history['val_loss'][-5:])\n",
    "    results.append((params, val_loss))\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Resultados\n",
    "results.sort(key=lambda x: x[1])\n",
    "print(\"\\n Results\")\n",
    "for rank, (params, loss) in enumerate(results, 1):\n",
    "    print(f\"{rank:2d}) Val Loss={loss:.4f} | Params={params}\")\n",
    "\n",
    "best_params, best_loss = results[0]\n",
    "print(\"\\n Mejor configuracion:\")\n",
    "print(best_params)\n",
    "print(f\"Validation Loss: {best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algunas referencias\n",
    "\n",
    "- https://keras.io/guides/sequential_model/\n",
    "- https://keras.io/api/layers/activations/\n",
    "- https://keras.io/optimizers/\n",
    "- https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0.003&noise=20&networkShape=4&seed=0.82538&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
